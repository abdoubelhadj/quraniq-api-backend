import os
import json
import re
import numpy as np
import faiss
import google.generativeai as genai
import logging
import requests
from openai import OpenAI # Importez le client OpenAI

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class QuranIQChatbot:
    def __init__(self):
        self.index = None
        self.chunks = []
        self.metadata = []
        self.gemini_model = None
        self.working_model_name = None
        self.is_loaded = False
        self.openai_client = None # Client OpenAI
        self.load_components()

    def find_working_gemini_model(self):
        """Trouve un mod√®le Gemini fonctionnel en testant les mod√®les disponibles."""
        models = ["gemini-1.5-flash", "gemini-1.5-pro", "gemini-pro", "models/gemini-1.5-flash", "models/gemini-1.5-pro", "models/gemini-pro"]
        for name in models:
            try:
                model = genai.GenerativeModel(name)
                model.generate_content("ping")
                logging.info(f"Found working Gemini model: {name}")
                return model, name
            except Exception as e:
                logging.warning(f"Model {name} failed to initialize or respond: {e}")
                continue
        return None, None

    def load_components(self):
        """Charge tous les composants n√©cessaires au chatbot."""
        try:
            logging.info("üîÑ Chargement du chatbot (avec RAG via OpenAI Embeddings)...")

            # Get API Key from environment variable for Gemini
            gemini_api_key = os.getenv("GOOGLE_GENERATIVE_AI_API_KEY")
            if not gemini_api_key:
                raise ValueError("GOOGLE_GENERATIVE_AI_API_KEY environment variable not set.")
            genai.configure(api_key=gemini_api_key)

            # Initialiser le client OpenAI pour les embeddings
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                raise ValueError("OPENAI_API_KEY environment variable not set. Please configure it for OpenAI embeddings.")
            self.openai_client = OpenAI(api_key=openai_api_key)
            logging.info("OpenAI client initialized for embeddings.")

            # --- LOGIQUE POUR T√âL√âCHARGER DEPUIS VERCEL BLOB ---
            BLOB_INDEX_URL = os.getenv("BLOB_INDEX_URL")
            BLOB_METADATA_URL = os.getenv("BLOB_METADATA_URL")

            if not BLOB_INDEX_URL or not BLOB_METADATA_URL:
                raise ValueError("BLOB_INDEX_URL or BLOB_METADATA_URL environment variables not set. Please configure them.")

            # T√©l√©chargement de l'index FAISS
            logging.info(f"Downloading FAISS index from {BLOB_INDEX_URL}")
            index_response = requests.get(BLOB_INDEX_URL)
            index_response.raise_for_status()
            with open("/tmp/index.faiss", "wb") as f:
                f.write(index_response.content)
            self.index = faiss.read_index("/tmp/index.faiss")
            logging.info("FAISS index downloaded and loaded.")

            # T√©l√©chargement des m√©tadonn√©es
            logging.info(f"Downloading chunks metadata from {BLOB_METADATA_URL}")
            metadata_response = requests.get(BLOB_METADATA_URL)
            metadata_response.raise_for_status()
            data = metadata_response.json()
            self.chunks = data["chunks"]
            self.metadata = data["metadata"]
            logging.info("Chunks metadata downloaded and loaded.")
            # --- FIN DE LA LOGIQUE BLOB ---

            self.gemini_model, self.working_model_name = self.find_working_gemini_model()
            if not self.gemini_model:
                raise Exception("Aucun mod√®le Gemini valide n'a pu √™tre trouv√© ou initialis√©.")
            
            self.is_loaded = True
            logging.info("‚úÖ Chatbot charg√© avec succ√®s (avec RAG via OpenAI Embeddings).")
        except Exception as e:
            logging.error(f"‚ùå Erreur lors du chargement du chatbot : {e}", exc_info=True)
            self.is_loaded = False

    def detect_language(self, text):
        """D√©tecte la langue du texte (fr, ar, en, dz)."""
        arabic_chars = re.compile(r'[\u0600-\u06FF]')
        if arabic_chars.search(text):
            algerian_words = ['Ÿàÿßÿ¥', 'ŸÉŸäŸÅÿßÿ¥', 'ŸàŸäŸÜ', 'ÿπŸÑÿßÿ¥', 'ÿ®ÿµÿ≠', 'ÿ®ÿ±ŸÉ', 'ÿ≠ŸÜÿß', 'ŸÜÿ™ŸàŸÖÿß', 'ŸáŸàŸÖÿß', 'ÿ±ÿßŸÜŸä', 'ÿ±ÿßŸÉ', 'ÿ±ÿßŸáÿß', 'ÿ™ÿßÿπ', 'ÿ®ÿ≤ÿßŸÅ', 'ÿ¥ŸàŸäÿ©']
            if any(word in text for word in algerian_words):
                return "dz"
            return "ar"
        
        english_words = ['what', 'how', 'why', 'when', 'where', 'who', 'is', 'are', 'the', 'and', 'or', 'can', 'should', 'must', 'do', 'did']
        if any(word in text.lower().split() for word in english_words):
            return "en"
        
        return "fr"

    def is_religious_question(self, query):
        """V√©rifie si la question est de nature religieuse en utilisant le mod√®le Gemini."""
        try:
            logging.info(f"Classifying question '{query[:50]}...' as religious or not using Gemini.")
            classification_prompt = f"""
            La question suivante est-elle de nature religieuse (Islam) ? R√©pondez uniquement par "OUI" ou "NON".
            Question: "{query}"
            """
            response = self.gemini_model.generate_content(classification_prompt)
            classification = response.text.strip().upper()
            
            if "OUI" in classification:
                logging.info(f"Question '{query[:50]}...' classified as RELIGIOUS.")
                return True
            else:
                logging.info(f"Question '{query[:50]}...' classified as NON-RELIGIOUS.")
                return False
        except Exception as e:
            logging.error(f"Erreur lors de la classification de la question par Gemini : {e}", exc_info=True)
            # En cas d'erreur, par d√©faut, nous la traitons comme non religieuse pour √©viter des boucles ou des r√©ponses non pertinentes.
            # Ou vous pouvez choisir de la traiter comme religieuse pour toujours essayer de r√©pondre.
            # Pour l'instant, nous allons la traiter comme non religieuse pour √©viter des co√ªts inutiles si l'API Gemini √©choue.
            return False

    def generate_query_embedding(self, query):
        """G√©n√®re l'embedding d'une requ√™te en utilisant OpenAI."""
        try:
            logging.info("Starting query embedding generation with OpenAI.")
            model_name = "text-embedding-ada-002" 
            response = self.openai_client.embeddings.create(input=[query], model=model_name)
            embedding = np.array(response.data[0].embedding).astype("float32").reshape(1, -1)
            logging.info("Query embedding generated successfully with OpenAI.")
            return embedding
        except Exception as e:
            logging.error(f"Error generating query embedding with OpenAI: {e}", exc_info=True)
            return None

    def search_similar_chunks(self, query, top_k=3):
        """Recherche les chunks les plus similaires dans l'index FAISS."""
        try:
            logging.info("Starting search for similar chunks.")
            emb = self.generate_query_embedding(query) # Appel √† OpenAI ici
            if emb is None:
                logging.warning("Embedding generation failed, returning empty chunks.")
                return []
            
            logging.info("Performing FAISS search.")
            distances, indices = self.index.search(emb, top_k)
            
            results = []
            for i, d in zip(indices[0], distances[0]):
                if 0 <= i < len(self.chunks):
                    results.append({
                        "chunk": self.chunks[i],
                        "source": self.metadata[i],
                        "distance": float(d)
                    })
            logging.info(f"FAISS search completed. Found {len(results)} relevant chunks.")
            return results
        except Exception as e:
            logging.error(f"Error searching similar chunks: {e}", exc_info=True)
            return []

    def generate_response(self, query, context_chunks, language):
        """G√©n√®re une r√©ponse en utilisant le mod√®le Gemini et le contexte RAG."""
        context = ""
        sources = []
        mode = "general"

        if context_chunks and context_chunks[0]['distance'] < 1.0: # Adjust threshold as needed
            context = "\n\n".join(f"Source: {c['source']}\nContenu: {c['chunk']}" for c in context_chunks[:2])
            sources = list(set(c['source'] for c in context_chunks[:2]))
            mode = "hybrid"
            logging.info("Context used for generation.")
        else:
            logging.info("No relevant context found or distance too high, generating general response.")

        prompts = {
            "fr": f"Tu es un expert de l'islam. R√©ponds clairement et de mani√®re concise. Utilise les informations fournies si pertinentes.\nQuestion : {query}\n{context}",
            "ar": f"ÿ£ŸÜÿ™ ÿÆÿ®Ÿäÿ± ŸÅŸä ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖ. ÿ£ÿ¨ÿ® ÿ®Ÿàÿ∂ÿ∂Ÿàÿ≠ Ÿàÿ•Ÿäÿ¨ÿßÿ≤. ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÖŸÇÿØŸÖÿ© ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿ∞ÿßÿ™ ÿµŸÑÿ©.\nÿßŸÑÿ≥ÿ§ÿßŸÑ: {query}\n{context}",
            "en": f"You are an expert in Islam. Answer clearly and concisely. Use provided information if relevant.\nQuestion: {query}\n{context}",
            "dz": f"ÿ±ÿßŸÉ ÿÆÿ®Ÿäÿ± ŸÅÿßŸÑÿØŸäŸÜ. ÿ¨ÿßŸàÿ® ÿ®ÿ®ÿ≥ÿßÿ∑ÿ© ŸàŸàÿ∂Ÿàÿ≠. ÿßÿ≥ÿ™ÿπŸÖŸÑ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÑŸä ÿπÿ∑Ÿäÿ™ŸÉ ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ŸÖŸÅŸäÿØÿ©.\nÿßŸÑÿ≥ÿ§ÿßŸÑ: {query}\n{context}",
        }

        prompt = prompts.get(language, prompts["fr"])
        logging.info(f"Sending prompt to Gemini model. Language: {language}, Mode: {mode}")
        try:
            result = self.gemini_model.generate_content(prompt)
            logging.info("Response received from Gemini.")
            return {
                "response": result.text.strip(),
                "language": language,
                "sources": sources,
                "mode": mode
            }
        except Exception as e:
            logging.error(f"Error generating content from Gemini: {e}", exc_info=True)
            return {
                "response": "D√©sol√©, une erreur est survenue lors de la g√©n√©ration de la r√©ponse.",
                "language": language,
                "sources": [],
                "mode": "error"
            }

    def chat(self, query):
        """Fonction principale de chat, g√®re la d√©tection de langue, la pertinence et la g√©n√©ration de r√©ponse."""
        logging.info(f"Chat request received: {query[:50]}...")
        lang = self.detect_language(query)
        logging.info(f"Detected language: {lang}")

        # Utiliser la nouvelle m√©thode de d√©tection bas√©e sur Gemini
        if not self.is_religious_question(query):
            logging.info("Non-religious question detected by Gemini.")
            return {
                "response": "Je suis QuranIQ, sp√©cialis√© uniquement dans les questions islamiques. Posez-moi une question sur l'Islam.",
                "language": lang,
                "sources": [],
                "mode": "non-religious"
            }
        
        logging.info("Religious question detected by Gemini. Searching for similar chunks...")
        chunks = self.search_similar_chunks(query)
        logging.info(f"Found {len(chunks)} similar chunks.")
        
        logging.info("Generating response from Gemini model...")
        return self.generate_response(query, chunks, lang)
