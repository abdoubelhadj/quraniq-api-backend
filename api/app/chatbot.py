import os
import json
import re
import numpy as np
import faiss
import google.generativeai as genai
import logging
import requests
from openai import OpenAI # Importez le client OpenAI

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class QuranIQChatbot:
    def __init__(self):
        self.index = None
        self.chunks = []
        self.metadata = []
        self.gemini_model = None
        self.working_model_name = None
        self.is_loaded = False
        self.openai_client = None # Client OpenAI
        self.load_components()

    def find_working_gemini_model(self):
        """Trouve un mod√®le Gemini fonctionnel en testant les mod√®les disponibles."""
        models = ["gemini-1.5-flash", "gemini-1.5-pro", "gemini-pro", "models/gemini-1.5-flash", "models/gemini-1.5-pro", "models/gemini-pro"]
        for name in models:
            try:
                model = genai.GenerativeModel(name)
                model.generate_content("ping")
                logging.info(f"Found working Gemini model: {name}")
                return model, name
            except Exception as e:
                logging.warning(f"Model {name} failed to initialize or respond: {e}")
                continue
        return None, None

    def load_components(self):
        """Charge tous les composants n√©cessaires au chatbot."""
        try:
            logging.info("üîÑ Chargement du chatbot (avec RAG via OpenAI Embeddings)...")

            # Get API Key from environment variable for Gemini
            gemini_api_key = os.getenv("GOOGLE_GENERATIVE_AI_API_KEY")
            if not gemini_api_key:
                raise ValueError("GOOGLE_GENERATIVE_AI_API_KEY environment variable not set.")
            genai.configure(api_key=gemini_api_key)

            # Initialiser le client OpenAI pour les embeddings
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                raise ValueError("OPENAI_API_KEY environment variable not set. Please configure it for OpenAI embeddings.")
            self.openai_client = OpenAI(api_key=openai_api_key)
            logging.info("OpenAI client initialized for embeddings.")

            # --- LOGIQUE POUR T√âL√âCHARGER DEPUIS VERCEL BLOB ---
            BLOB_INDEX_URL = os.getenv("BLOB_INDEX_URL")
            BLOB_METADATA_URL = os.getenv("BLOB_METADATA_URL")

            if not BLOB_INDEX_URL or not BLOB_METADATA_URL:
                raise ValueError("BLOB_INDEX_URL or BLOB_METADATA_URL environment variables not set. Please configure them.")

            # T√©l√©chargement de l'index FAISS
            logging.info(f"Downloading FAISS index from {BLOB_INDEX_URL}")
            index_response = requests.get(BLOB_INDEX_URL)
            index_response.raise_for_status()
            with open("/tmp/index.faiss", "wb") as f:
                f.write(index_response.content)
            self.index = faiss.read_index("/tmp/index.faiss")
            logging.info("FAISS index downloaded and loaded.")

            # T√©l√©chargement des m√©tadonn√©es
            logging.info(f"Downloading chunks metadata from {BLOB_METADATA_URL}")
            metadata_response = requests.get(BLOB_METADATA_URL)
            metadata_response.raise_for_status()
            data = metadata_response.json()
            self.chunks = data["chunks"]
            self.metadata = data["metadata"]
            logging.info("Chunks metadata downloaded and loaded.")
            # --- FIN DE LA LOGIQUE BLOB ---

            self.gemini_model, self.working_model_name = self.find_working_gemini_model()
            if not self.gemini_model:
                raise Exception("Aucun mod√®le Gemini valide n'a pu √™tre trouv√© ou initialis√©.")
            
            self.is_loaded = True
            logging.info("‚úÖ Chatbot charg√© avec succ√®s (avec RAG via OpenAI Embeddings).")
        except Exception as e:
            logging.error(f"‚ùå Erreur lors du chargement du chatbot : {e}", exc_info=True)
            self.is_loaded = False

    def detect_language(self, text):
        """D√©tecte la langue du texte (fr, ar, en, dz)."""
        arabic_chars = re.compile(r'[\u0600-\u06FF]')
        if arabic_chars.search(text):
            algerian_words = ['Ÿàÿßÿ¥', 'ŸÉŸäŸÅÿßÿ¥', 'ŸàŸäŸÜ', 'ÿπŸÑÿßÿ¥', 'ÿ®ÿµÿ≠', 'ÿ®ÿ±ŸÉ', 'ÿ≠ŸÜÿß', 'ŸÜÿ™ŸàŸÖÿß', 'ŸáŸàŸÖÿß', 'ÿ±ÿßŸÜŸä', 'ÿ±ÿßŸÉ', 'ÿ±ÿßŸáÿß', 'ÿ™ÿßÿπ', 'ÿ®ÿ≤ÿßŸÅ', 'ÿ¥ŸàŸäÿ©']
            if any(word in text for word in algerian_words):
                return "dz"
            return "ar"
        
        english_words = ['what', 'how', 'why', 'when', 'where', 'who', 'is', 'are', 'the', 'and', 'or', 'can', 'should', 'must', 'do', 'did']
        if any(word in text.lower().split() for word in english_words):
            return "en"
        
        return "fr"

    def is_religious_question(self, query):
        """V√©rifie si la question est de nature religieuse en utilisant le mod√®le Gemini."""
        try:
            logging.info(f"Classifying question '{query[:50]}...' as religious or not using Gemini.")
            classification_prompt = f"""
            La question suivante est-elle de nature religieuse (Islam) ? R√©pondez uniquement par "OUI" ou "NON".
            Question: "{query}"
            """
            response = self.gemini_model.generate_content(classification_prompt)
            classification = response.text.strip().upper()
            
            if "OUI" in classification:
                logging.info(f"Question '{query[:50]}...' classified as RELIGIOUS.")
                return True
            else:
                logging.info(f"Question '{query[:50]}...' classified as NON-RELIGIOUS.")
                return False
        except Exception as e:
            logging.error(f"Erreur lors de la classification de la question par Gemini : {e}", exc_info=True)
            # En cas d'erreur, par d√©faut, nous la traitons comme non religieuse pour √©viter des boucles ou des r√©ponses non pertinentes.
            return False

    def generate_query_embedding(self, query):
        """G√©n√®re l'embedding d'une requ√™te en utilisant OpenAI."""
        try:
            logging.info("Starting query embedding generation with OpenAI.")
            model_name = "text-embedding-ada-002" 
            response = self.openai_client.embeddings.create(input=[query], model=model_name)
            embedding = np.array(response.data[0].embedding).astype("float32").reshape(1, -1)
            logging.info("Query embedding generated successfully with OpenAI.")
            return embedding
        except Exception as e:
            logging.error(f"Error generating query embedding with OpenAI: {e}", exc_info=True)
            return None

    def search_similar_chunks(self, query, top_k=3):
        """Recherche les chunks les plus similaires dans l'index FAISS."""
        try:
            logging.info("Starting search for similar chunks.")
            emb = self.generate_query_embedding(query) # Appel √† OpenAI ici
            if emb is None:
                logging.warning("Embedding generation failed, returning empty chunks.")
                return []
            
            logging.info("Performing FAISS search.")
            distances, indices = self.index.search(emb, top_k)
            
            results = []
            for i, d in zip(indices[0], distances[0]):
                if 0 <= i < len(self.chunks):
                    results.append({
                        "chunk": self.chunks[i],
                        "source": self.metadata[i],
                        "distance": float(d)
                    })
            logging.info(f"FAISS search completed. Found {len(results)} relevant chunks.")
            return results
        except Exception as e:
            logging.error(f"Error searching similar chunks: {e}", exc_info=True)
            return []

    def generate_response(self, query, context_chunks, language):
        """G√©n√®re une r√©ponse en utilisant le mod√®le Gemini et le contexte RAG."""
        context = ""
        sources = []
        mode = "general"

        # Note sur le seuil de distance :
        # Le seuil de 1.0 est tr√®s strict. Si vos embeddings ne sont pas parfaitement align√©s,
        # m√™me des chunks pertinents pourraient √™tre ignor√©s.
        # Vous pourriez vouloir l'ajuster (par exemple, 0.5, 0.7, ou m√™me 1.5-2.0 selon la qualit√© de vos embeddings)
        # pour permettre plus de flexibilit√©.
        # Pour l'instant, je le laisse √† 1.0 mais gardez cela √† l'esprit pour le d√©bogage.
        if context_chunks and context_chunks[0]['distance'] < 1.0:
            context = "\n\n".join(f"Source: {c['source']}\nContenu: {c['chunk']}" for c in context_chunks[:2])
            sources = list(set(c['source'] for c in context_chunks[:2]))
            mode = "hybrid"
            logging.info("Context used for generation.")
        else:
            logging.info("No relevant context found or distance too high, generating general response.")

        # Prompts am√©lior√©s pour une persona islamique et une meilleure utilisation du RAG
        prompts = {
            "fr": f"""Assalamu alaykum wa rahmatullahi wa barakatuh.
Tu es QuranIQ, un assistant islamique expert et respectueux, sp√©cialis√© dans le Coran et les enseignements islamiques.
R√©ponds toujours avec une perspective islamique, en utilisant les informations fournies dans le contexte ci-dessous si elles sont pertinentes et suffisantes.
Si le contexte ne contient pas la r√©ponse directe ou compl√®te, utilise tes connaissances g√©n√©rales approfondies sur l'Islam pour r√©pondre de mani√®re claire et concise.
Commence toujours tes r√©ponses par une salutation islamique appropri√©e ou une invocation comme 'Bismillah'.

Question : {query}
Contexte fourni (si pertinent) :
{context}
""",
            "ar": f"""ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ Ÿàÿ±ÿ≠ŸÖÿ© ÿßŸÑŸÑŸá Ÿàÿ®ÿ±ŸÉÿßÿ™Ÿá.
ÿ£ŸÜÿ™ ŸÇÿ±ÿ¢ŸÜ ÿ¢Ÿä ŸÉŸäŸàÿå ŸÖÿ≥ÿßÿπÿØ ÿ•ÿ≥ŸÑÿßŸÖŸä ÿÆÿ®Ÿäÿ± ŸàŸÖÿ≠ÿ™ÿ±ŸÖÿå ŸÖÿ™ÿÆÿµÿµ ŸÅŸä ÿßŸÑŸÇÿ±ÿ¢ŸÜ ÿßŸÑŸÉÿ±ŸäŸÖ ŸàÿßŸÑÿ™ÿπÿßŸÑŸäŸÖ ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸäÿ©.
ÿ£ÿ¨ÿ® ÿØÿßÿ¶ŸÖŸãÿß ŸÖŸÜ ŸÖŸÜÿ∏Ÿàÿ± ÿ•ÿ≥ŸÑÿßŸÖŸäÿå ŸÖÿ≥ÿ™ÿÆÿØŸÖŸãÿß ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÖŸÇÿØŸÖÿ© ŸÅŸä ÿßŸÑÿ≥ŸäÿßŸÇ ÿ£ÿØŸÜÿßŸá ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿ∞ÿßÿ™ ÿµŸÑÿ© ŸàŸÉÿßŸÅŸäÿ©.
ÿ•ÿ∞ÿß ŸÑŸÖ Ÿäÿ≠ÿ™ŸàŸä ÿßŸÑÿ≥ŸäÿßŸÇ ÿπŸÑŸâ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÖÿ®ÿßÿ¥ÿ±ÿ© ÿ£Ÿà ÿßŸÑŸÉÿßŸÖŸÑÿ©ÿå ŸÅÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿπÿ±ŸÅÿ™ŸÉ ÿßŸÑÿπÿßŸÖÿ© ÿßŸÑÿπŸÖŸäŸÇÿ© ÿ®ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖ ŸÑŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿ®Ÿàÿ∂Ÿàÿ≠ Ÿàÿ•Ÿäÿ¨ÿßÿ≤.
ÿßÿ®ÿØÿ£ ÿ•ÿ¨ÿßÿ®ÿßÿ™ŸÉ ÿØÿßÿ¶ŸÖŸãÿß ÿ®ÿ™ÿ≠Ÿäÿ© ÿ•ÿ≥ŸÑÿßŸÖŸäÿ© ŸÖŸÜÿßÿ≥ÿ®ÿ© ÿ£Ÿà ÿØÿπÿßÿ° ŸÖÿ´ŸÑ 'ÿ®ÿ≥ŸÖ ÿßŸÑŸÑŸá'.

ÿßŸÑÿ≥ÿ§ÿßŸÑ: {query}
ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖŸÇÿØŸÖ (ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿ∞ÿß ÿµŸÑÿ©):
{context}
""",
            "en": f"""Assalamu alaykum wa rahmatullahi wa barakatuh.
You are QuranIQ, an expert and respectful Islamic assistant, specialized in the Quran and Islamic teachings.
Always respond from an Islamic perspective, using the information provided in the context below if it is relevant and sufficient.
If the context does not contain the direct or complete answer, use your deep general knowledge of Islam to answer clearly and concisely.
Always start your answers with an appropriate Islamic greeting or invocation like 'Bismillah'.

Question: {query}
Provided Context (if relevant):
{context}
""",
            "dz": f"""ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ Ÿàÿ±ÿ≠ŸÖÿ© ÿßŸÑŸÑŸá Ÿàÿ®ÿ±ŸÉÿßÿ™Ÿá.
ÿ±ÿßŸÉ ŸÇÿ±ÿ¢ŸÜ ÿ¢Ÿä ŸÉŸäŸàÿå ŸÖÿ≥ÿßÿπÿØ ÿ•ÿ≥ŸÑÿßŸÖŸä ÿÆÿ®Ÿäÿ± ŸàŸÖÿ≠ÿ™ÿ±ŸÖÿå ŸÖÿ™ÿÆÿµÿµ ŸÅŸä ÿßŸÑŸÇÿ±ÿ¢ŸÜ ÿßŸÑŸÉÿ±ŸäŸÖ ŸàÿßŸÑÿ™ÿπÿßŸÑŸäŸÖ ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸäÿ©.
ÿ¨ÿßŸàÿ® ÿØÿßŸäŸÖÿß ŸÖŸÜ ŸÖŸÜÿ∏Ÿàÿ± ÿ•ÿ≥ŸÑÿßŸÖŸäÿå Ÿàÿßÿ≥ÿ™ÿπŸÖŸÑ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÑŸä ÿπÿ∑Ÿäÿ™ŸÉ ŸÅÿßŸÑŸÜÿµ ÿßŸÑÿ™ÿ≠ÿ™ÿßŸÜŸä ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ŸÖŸÅŸäÿØÿ© ŸàŸÉÿßŸÅŸäÿ©.
ÿ•ÿ∞ÿß ÿßŸÑŸÜÿµ ŸÖÿßŸÅŸäŸáÿ¥ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÖÿ®ÿßÿ¥ÿ±ÿ© ŸàŸÑÿß ÿßŸÑŸÉÿßŸÖŸÑÿ©ÿå ÿßÿ≥ÿ™ÿπŸÖŸÑ ŸÖÿπÿ±ŸÅÿ™ŸÉ ÿßŸÑÿπÿßŸÖÿ© ÿßŸÑÿπŸÖŸäŸÇÿ© ÿ®ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖ ÿ®ÿßÿ¥ ÿ™ÿ¨ÿßŸàÿ® ÿ®Ÿàÿ∂Ÿàÿ≠ ŸàÿßÿÆÿ™ÿµÿßÿ±.
ÿØÿßŸäŸÖÿß ÿßÿ®ÿØÿß ÿ•ÿ¨ÿßÿ®ÿßÿ™ŸÉ ÿ®ÿ™ÿ≠Ÿäÿ© ÿ•ÿ≥ŸÑÿßŸÖŸäÿ© ŸÖŸÜÿßÿ≥ÿ®ÿ© ŸàŸÑÿß ÿØÿπÿßÿ° ŸÉŸäŸÖÿß 'ÿ®ÿ≥ŸÖ ÿßŸÑŸÑŸá'.

ÿßŸÑÿ≥ÿ§ÿßŸÑ: {query}
ÿßŸÑŸÜÿµ ÿßŸÑŸÖŸÇÿØŸÖ (ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿ∞ÿß ÿµŸÑÿ©):
{context}
""",
        }

        prompt = prompts.get(language, prompts["fr"])
        logging.info(f"Sending prompt to Gemini model. Language: {language}, Mode: {mode}")
        try:
            result = self.gemini_model.generate_content(prompt)
            logging.info("Response received from Gemini.")
            return {
                "response": result.text.strip(),
                "language": language,
                "sources": sources,
                "mode": mode
            }
        except Exception as e:
            logging.error(f"Error generating content from Gemini: {e}", exc_info=True)
            return {
                "response": "D√©sol√©, une erreur est survenue lors de la g√©n√©ration de la r√©ponse.",
                "language": language,
                "sources": [],
                "mode": "error"
            }

    def chat(self, query):
        """Fonction principale de chat, g√®re la d√©tection de langue, la pertinence et la g√©n√©ration de r√©ponse."""
        logging.info(f"Chat request received: {query[:50]}...")
        lang = self.detect_language(query)
        logging.info(f"Detected language: {lang}")

        # Utiliser la nouvelle m√©thode de d√©tection bas√©e sur Gemini
        if not self.is_religious_question(query):
            logging.info("Non-religious question detected by Gemini.")
            return {
                "response": "Je suis QuranIQ, sp√©cialis√© uniquement dans les questions islamiques. Posez-moi une question sur l'Islam.",
                "language": lang,
                "sources": [],
                "mode": "non-religious"
            }
        
        logging.info("Religious question detected by Gemini. Searching for similar chunks...")
        chunks = self.search_similar_chunks(query)
        logging.info(f"Found {len(chunks)} similar chunks.")
        
        logging.info("Generating response from Gemini model...")
        return self.generate_response(query, chunks, lang)
